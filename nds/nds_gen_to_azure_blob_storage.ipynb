{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7484c2d8-910d-4be1-a251-3c6fa8fe1a0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### copy tpc-ds tools including dsdgen to cwd and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad73192-6f87-40e7-8c55-002da972cdff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "! cp -rf /dbfs/user/eordentlich/nds/tools tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8b6b81-6283-4964-ae2b-d41fbd1454dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsdgen\ttpcds.idx\r\n"
     ]
    }
   ],
   "source": [
    "! ls tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84f27de-5e3f-4832-896b-7d475cea6d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsdgen Population Generator (Version 3.2.0)\r\nCopyright Transaction Processing Performance Council (TPC) 2001 - 2021\r\nWarning: This scale factor is valid for QUALIFICATION ONLY\r\n"
     ]
    }
   ],
   "source": [
    "! cd tools && ./dsdgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a70f198-3399-45c5-9b22-0bc2c4c41e53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_center.dat\t\t   dsdgen\t\t       store_returns.dat\r\ncatalog_page.dat\t   household_demographics.dat  store_sales.dat\r\ncatalog_returns.dat\t   income_band.dat\t       time_dim.dat\r\ncatalog_sales.dat\t   inventory.dat\t       tpcds.idx\r\ncustomer.dat\t\t   item.dat\t\t       warehouse.dat\r\ncustomer_address.dat\t   promotion.dat\t       web_page.dat\r\ncustomer_demographics.dat  reason.dat\t\t       web_returns.dat\r\ndate_dim.dat\t\t   ship_mode.dat\t       web_sales.dat\r\ndbgen_version.dat\t   store.dat\t\t       web_site.dat\r\n"
     ]
    }
   ],
   "source": [
    "! ls tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f710a368-cd1d-4da7-96fa-9509e83d9d93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### generate nds scale factor parts in parallel using pyspark udf and save to dbfs via local mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b478b2-a05e-42e1-a823-14c11fe67aa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_dsdgen(parallel, child, scale):\n",
    "  import shutil\n",
    "  import subprocess\n",
    "  import os\n",
    "  shutil.copytree('/dbfs/user/eordentlich/nds/tools', f'tools_{child}')\n",
    "  os.chdir(f'tools_{child}')\n",
    "  cmd = f'./dsdgen -parallel {parallel} -child {child} -scale {scale} -dir /dbfs/user/eordentlich/nds/sf{scale}'.split(\" \")\n",
    "  print(cmd)\n",
    "  subprocess.run(cmd)\n",
    "  os.chdir('..')\n",
    "  shutil.rmtree(f'tools_{child}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0e37639-f25a-4829-bdc0-d429d485ad0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "sf 1000 with 100 fold parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb7c82a-d002-49b1-aead-a2880476a4c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.parallelize(list(range(1,101)),100).foreach(lambda c: run_dsdgen(100, c, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d9c4a7a-e16f-4168-ae96-301b087e442e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "sf 10000 with 100 fold parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e38ce9-c51a-46f0-b8ff-81d865b9c7b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.parallelize(list(range(1,101)),100).foreach(lambda c: run_dsdgen(100, c, 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c61dd6f-a6c8-4611-96f1-55b66f245164",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### convert to parquet, saving result to mahrens container in azure blob storage\n",
    "uses (slightly modified) scripts from: https://github.com/NVIDIA/spark-rapids-benchmarks/blob/dev/nds/ which should be copied to workspace location of this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e93ed5bd-9985-4e15-88b4-7c2e332d545e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Configure credentials for mahrens account and container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720e26e5-a610-4d90-9683-ef13b8ceea32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage Account Name\n",
    "storage_account_name = \"mahrens\"\n",
    "\n",
    "# Azure Storage Account Key\n",
    "storage_account_key = dbutils.secrets.get(scope=\"mahrens-azure\", key=\"storage_account_access_key\")\n",
    "\n",
    "# Azure Storage Account Source Container\n",
    "container = \"mahrens\"\n",
    "\n",
    "# Set the configuration details to read/write\n",
    "spark.conf.set(\"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name), storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8bba6b-2d6c-493d-bf2b-b00af39aa5b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nds_schema import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4982909-43aa-4bf9-b138-580e60dc9b97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--database'], dest='database', nargs=None, const=None, default='default', type=None, choices=None, required=False, help='the name of a database to use instead of `default`, currently applies only to Hive', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    'input_prefix',\n",
    "    help='text to prepend to every input file path (e.g., \"hdfs:///ds-generated-data\"; the default is empty)')\n",
    "parser.add_argument(\n",
    "    'output_prefix',\n",
    "    help='text to prepend to every output file (e.g., \"hdfs:///ds-parquet\"; the default is empty)' +\n",
    "    '. If output_format is \"iceberg\", this argument will be regarded as the value of property ' +\n",
    "    '\"spark.sql.catalog.spark_catalog.warehouse\". Only default Spark catalog ' +\n",
    "    'session name \"spark_catalog\" is supported now, customized catalog is not ' +\n",
    "    'yet supported.')\n",
    "parser.add_argument(\n",
    "    'report_file',\n",
    "    help='location to store a performance report(local)')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--output_mode',\n",
    "    choices=['overwrite', 'append', 'ignore', 'error', 'errorifexists'],\n",
    "    help=\"save modes as defined by \" +\n",
    "    \"https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes.\" +\n",
    "    \"default value is errorifexists, which is the Spark default behavior.\",\n",
    "    default=\"errorifexists\")\n",
    "parser.add_argument(\n",
    "    '--input_format',\n",
    "    choices=['csv', 'parquet', 'orc', 'avro', 'json'],\n",
    "    default='csv',\n",
    "    help='input data format to be converted. default value is csv.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--output_format',\n",
    "    choices=['parquet', 'orc', 'avro', 'json', 'iceberg', 'delta'],\n",
    "    default='parquet',\n",
    "    help=\"output data format when converting CSV data sources.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--tables',\n",
    "    type=lambda s: s.split(','),\n",
    "    help=\"specify table names by a comma separated string. e.g. 'catalog_page,catalog_sales'.\")\n",
    "parser.add_argument(\n",
    "    '--log_level',\n",
    "    help='set log level for Spark driver log. Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN(default: INFO)',\n",
    "    default=\"INFO\")\n",
    "parser.add_argument(\n",
    "    '--floats',\n",
    "    action='store_true',\n",
    "    help='replace DecimalType with DoubleType when saving parquet files. If not specified, decimal data will be saved.')\n",
    "parser.add_argument(\n",
    "    '--update',\n",
    "    action='store_true',\n",
    "    help='transcode the source data or update data'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--iceberg_write_format',\n",
    "    choices=['parquet', 'orc', 'avro'],\n",
    "    default='parquet',\n",
    "    help='File format for the Iceberg table; parquet, avro, or orc'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--compression',\n",
    "    help='Compression codec to use when saving data.' +\n",
    "    ' See https://iceberg.apache.org/docs/latest/configuration/#write-properties ' +\n",
    "    ' for supported codecs in Iceberg.' +\n",
    "    ' See https://spark.apache.org/docs/latest/sql-data-sources.html' +\n",
    "    ' for supported codecs for Spark built-in formats.' +\n",
    "    ' When not specified, the default for the requested output format will be used.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--delta_unmanaged',\n",
    "    action='store_true',\n",
    "    help='Use unmanaged tables for DeltaLake. This is useful for testing DeltaLake without ' +\n",
    "    'leveraging a Metastore service.')\n",
    "parser.add_argument(\n",
    "    '--hive',\n",
    "    action='store_true',\n",
    "    help='create Hive external tables for the converted data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--database',\n",
    "    help='the name of a database to use instead of `default`, currently applies only to Hive',\n",
    "    default=\"default\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc8c10c7-371f-4cd3-b2de-de1802129c0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Args for 10k sf case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad45f24c-643c-4103-a476-88cf3c1207d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"/user/eordentlich/nds/sf10000 wasbs://nds@mahrens.blob.core.windows.net/parquet_sf10000 /dbfs/user/eordentlich/nds/wasbs_parquet_sf10000_report\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2b5d3d-4f12-43f8-bfcf-2397ff233ed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nds_transcode import transcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "821a9fff-108c-4dfe-a324-d950311cd324",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(input_prefix='/user/eordentlich/nds/sf10000', output_prefix='wasbs://nds@mahrens.blob.core.windows.net/parquet_sf10000', report_file='/dbfs/user/eordentlich/nds/wasbs_parquet_sf10000_report', output_mode='errorifexists', input_format='csv', output_format='parquet', tables=None, log_level='INFO', floats=False, update=False, iceberg_write_format='parquet', compression=None, delta_unmanaged=False, hive=False, database='default')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f3d1755-1d2d-4e5b-8265-f9ce40504a6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Test Start Time: 2024-01-27 18:06:30.558722\nLoad Test Finished at: 2024-01-27 23:29:38.940507\nLoad Test Time: 19388.381785 seconds\nRNGSEED used :01272329389\nLoad Test Time: 19388.381785 seconds\nLoad Test Finished at: 2024-01-27 23:29:38.940507\nRNGSEED used: 01272329389\nTime to convert 'customer_address' was 457.8248s\nTime to convert 'customer_demographics' was 23.5905s\nTime to convert 'date_dim' was 2.6950s\nTime to convert 'warehouse' was 1.0044s\nTime to convert 'ship_mode' was 0.9606s\nTime to convert 'time_dim' was 3.7482s\nTime to convert 'reason' was 0.9968s\nTime to convert 'income_band' was 0.9910s\nTime to convert 'item' was 10.2177s\nTime to convert 'store' was 1.0172s\nTime to convert 'call_center' was 0.9178s\nTime to convert 'customer' was 1001.5614s\nTime to convert 'web_site' was 1.2925s\nTime to convert 'store_returns' was 1117.3322s\nTime to convert 'household_demographics' was 1.2715s\nTime to convert 'web_page' was 1.2282s\nTime to convert 'promotion' was 1.1282s\nTime to convert 'catalog_page' was 1.6290s\nTime to convert 'inventory' was 191.4873s\nTime to convert 'catalog_returns' was 366.2772s\nTime to convert 'web_returns' was 271.5344s\nTime to convert 'web_sales' was 1902.0814s\nTime to convert 'catalog_sales' was 3842.2425s\nTime to convert 'store_sales' was 10185.3439s\n\n\n\nSpark configuration follows:\n\n\n('spark.databricks.preemption.enabled', 'true')\n('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '1')\n('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*')\n('spark.driver.tempDirectory', '/local_disk0/tmp')\n('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse')\n('spark.databricks.managedCatalog.clientClassName', 'com.databricks.managedcatalog.ManagedCatalogClientImpl')\n('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName', 'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider')\n('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true')\n('spark.sql.streaming.checkpointFileManagerClass', 'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager')\n('spark.databricks.service.dbutils.repl.backend', 'com.databricks.dbconnect.ReplDBUtils')\n('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false')\n('spark.streaming.driver.writeAheadLog.allowBatching', 'true')\n('spark.databricks.clusterSource', 'UI')\n('spark.hadoop.hive.server2.transport.mode', 'http')\n('spark.databricks.acl.dfAclsEnabled', 'false')\n('spark.databricks.clusterUsageTags.driverContainerId', '6141f0666dff45039631ca9e66795e4d')\n('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true')\n('spark.databricks.clusterUsageTags.hailEnabled', 'false')\n('spark.databricks.clusterUsageTags.dataPlaneRegion', 'westus2')\n('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false')\n('spark.databricks.clusterUsageTags.containerType', 'LXC')\n('spark.hadoop.fs.s3a.assumed.role.credentials.provider', 'shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider')\n('spark.eventLog.enabled', 'false')\n('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false')\n('spark.hadoop.fs.stage.impl.disable.cache', 'true')\n('spark.hadoop.hive.hmshandler.retry.interval', '2000')\n('spark.executor.tempDirectory', '/local_disk0/tmp')\n('spark.hadoop.fs.azure.authorization.caching.enable', 'false')\n('spark.r.sql.derby.temp.dir', '/tmp/RtmpvLJzfV')\n('spark.hadoop.fs.fcfs-abfss.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.driver.port', '45973')\n('spark.hadoop.mapred.output.committer.class', 'com.databricks.backend.daemon.data.client.DirectOutputCommitter')\n('spark.databricks.clusterUsageTags.clusterTargetWorkers', '2')\n('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3', '0')\n('spark.sql.allowMultipleContexts', 'false')\n('spark.databricks.eventLog.enabled', 'true')\n('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '6905323869956636')\n('spark.hadoop.hive.server2.thrift.http.port', '10000')\n('spark.databricks.workerNodeTypeId', 'Standard_D8ads_v5')\n('spark.home', '/databricks/spark')\n('spark.hadoop.hive.server2.idle.operation.timeout', '7200000')\n('spark.task.reaper.enabled', 'true')\n('spark.databricks.clusterUsageTags.autoTerminationMinutes', '60')\n('spark.storage.memoryFraction', '0.5')\n('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1')\n('spark.databricks.sql.configMapperClass', 'com.databricks.dbsql.config.SqlConfigMapperBridge')\n('spark.driver.maxResultSize', '4g')\n('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false')\n('spark.hadoop.fs.fcfs-s3.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.databricks.delta.multiClusterWrites.enabled', 'true')\n('spark.databricks.clusterUsageTags.sparkImageLabel', 'release__13.3.x-snapshot-scala2.12__databricks-universe__13.3.9__74fadf9__af3efa4__jenkins__f771602__format-3')\n('spark.worker.cleanup.enabled', 'false')\n('spark.sql.legacy.createHiveTableByDefault', 'false')\n('spark.databricks.driver.preferredMavenCentralMirrorUrl', 'https://maven-central.storage-download.googleapis.com/maven2/')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File', '0')\n('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true')\n('spark.databricks.clusterUsageTags.clusterNodeType', 'Standard_D8ads_v5')\n('spark.ui.port', '40001')\n('spark.hadoop.fs.s3a.attempts.maximum', '10')\n('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false')\n('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false')\n('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true')\n('spark.hadoop.fs.azure.user.agent.prefix', '')\n('spark.databricks.driverNodeTypeId', 'Standard_D4ads_v5')\n('spark.hadoop.fs.s3n.impl.disable.cache', 'true')\n('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough', 'false')\n('spark.hadoop.fs.fcfs-s3n.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.hadoop.fs.abfs.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms')\n('spark.hadoop.fs.wasb.impl.disable.cache', 'true')\n('spark.databricks.clusterUsageTags.clusterLogDestination', '')\n('spark.databricks.wsfsPublicPreview', 'true')\n('spark.databricks.clusterUsageTags.clusterMinWorkers', '2')\n('spark.cleaner.referenceTracking.blocking', 'false')\n('spark.databricks.clusterUsageTags.clusterState', 'Pending')\n('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes', 'false')\n('spark.databricks.tahoe.logStore.azure.class', 'com.databricks.tahoe.store.AzureLogStore')\n('spark.hadoop.fs.azure.skip.metrics', 'true')\n('spark.hadoop.hive.hmshandler.retry.attempts', '10')\n('spark.hadoop.fs.wasb.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.scheduler.mode', 'FAIR')\n('spark.databricks.clusterUsageTags.clusterLastActivityTime', '1706352845177')\n('spark.sql.sources.default', 'delta')\n('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled', 'true')\n('spark.hadoop.fs.cpfs-s3n.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem')\n('spark.hadoop.fs.cpfs-adl.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem')\n('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true')\n('spark.hadoop.fs.cpfs-abfss.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem')\n('spark.databricks.clusterUsageTags.clusterMaxWorkers', '16')\n('spark.databricks.passthrough.oauth.refresher.impl', 'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient')\n('spark.sql.hive.metastore.sharedPrefixes', 'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks')\n('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false')\n('spark.task.reaper.killTimeout', '60s')\n('spark.hadoop.parquet.block.size.row.check.min', '10')\n('spark.hadoop.hive.server2.use.SSL', 'true')\n('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'vnet-injection')\n('spark.hadoop.spark.databricks.metrics.filesystem_metrics', 'true')\n('spark.hadoop.databricks.dbfs.client.version', 'v2')\n('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0')\n('spark.hadoop.hive.server2.keystore.path', '/databricks/keys/jetty-ssl-driver-keystore.jks')\n('spark.hadoop.fs.gs.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.databricks.credential.redactor', 'com.databricks.logging.secrets.CredentialRedactorProxyImpl')\n('spark.databricks.clusterUsageTags.clusterPinned', 'false')\n('spark.databricks.sparkContextId', '4902740931744109684')\n('spark.databricks.acl.provider', 'com.databricks.sql.acl.ReflectionBackedAclProvider')\n('spark.databricks.wsfs.workspacePrivatePreview', 'true')\n('spark.databricks.mlflow.autologging.enabled', 'true')\n('spark.extraListeners', 'com.databricks.backend.daemon.driver.DBCEventLoggingListener')\n('spark.databricks.clusterUsageTags.azureSubscriptionId', 'b12f9ba9-cfc2-463a-8f57-b41db30bc6df')\n('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled', 'false')\n('spark.sql.parquet.cacheMetadata', 'true')\n('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss', '0')\n('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true')\n('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true')\n('spark.databricks.clusterUsageTags.userProvidedSparkVersion', '13.3.x-scala2.12')\n('spark.hadoop.fs.abfss.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false')\n('spark.databricks.tahoe.logStore.class', 'com.databricks.tahoe.store.DelegatingLogStore')\n('spark.hadoop.fs.s3.impl.disable.cache', 'true')\n('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30')\n('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30')\n('libraryDownload.sleepIntervalSeconds', '5')\n('spark.sql.hive.convertMetastoreParquet', 'true')\n('spark.databricks.service.dbutils.server.backend', 'com.databricks.dbconnect.SparkServerDBUtils')\n('spark.executor.id', 'driver')\n('spark.databricks.clusterUsageTags.region', 'westus2')\n('spark.databricks.repl.enableClassFileCleanup', 'true')\n('spark.databricks.clusterUsageTags.driverContainerPrivateIp', '10.2.128.10')\n('spark.databricks.clusterUsageTags.userId', '8796140328588637')\n('spark.databricks.clusterUsageTags.orgId', '6905323869956636')\n('spark.hadoop.fs.s3a.multipart.size', '10485760')\n('spark.metrics.conf', '/databricks/spark/conf/metrics.properties')\n('spark.databricks.clusterUsageTags.effectiveSparkVersion', '13.3.x-scala2.12')\n('spark.akka.frameSize', '256')\n('spark.databricks.unityCatalog.credentialManager.enableDriverExecutorSharedCredCache', 'false')\n('spark.hadoop.fs.s3a.fast.upload', 'true')\n('spark.repl.class.outputDir', '/local_disk0/tmp/repl/spark-4902740931744109684-52b63114-97d8-4388-ba70-cc2b2a0de761')\n('spark.sql.streaming.stopTimeout', '15s')\n('spark.hadoop.hive.server2.keystore.password', '[REDACTED]')\n('spark.databricks.clusterUsageTags.clusterAllTags', '[{\"key\":\"Vendor\",\"value\":\"Databricks\"},{\"key\":\"Creator\",\"value\":\"eordentlich@nvidia.com\"},{\"key\":\"ClusterName\",\"value\":\"eo-general-purpose\"},{\"key\":\"ClusterId\",\"value\":\"0126-064856-zlv8jbbc\"},{\"key\":\"DatabricksEnvironment\",\"value\":\"workerenv-6905323869956636\"}]')\n('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting', 'false')\n('spark.hadoop.fs.s3a.retry.interval', '250ms')\n('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false')\n('spark.databricks.overrideDefaultCommitProtocol', 'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol')\n('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass', 'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient')\n('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false')\n('spark.hadoop.fs.adl.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('libraryDownload.timeoutSeconds', '180')\n('spark.hadoop.parquet.memory.pool.ratio', '0.5')\n('spark.databricks.clusterUsageTags.driverInstancePrivateIp', '10.2.64.15')\n('spark.databricks.passthrough.adls.gen2.tokenProviderClassName', 'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider')\n('spark.databricks.unityCatalog.legacy.enableCrossScopeCredCache', 'true')\n('spark.hadoop.fs.s3a.block.size', '67108864')\n('spark.executor.memory', '21588m')\n('spark.databricks.tahoe.logStore.gcp.class', 'com.databricks.tahoe.store.GCPLogStore')\n('spark.serializer.objectStreamReset', '100')\n('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None')\n('spark.databricks.passthrough.enabled', 'false')\n('spark.sql.sources.commitProtocolClass', 'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs', '0')\n('spark.hadoop.fs.fcfs-s3a.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.databricks.clusterUsageTags.attribute_tag_budget', '')\n('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType', 'azure_disk_volume_type: PREMIUM_LRS\\n')\n('spark.databricks.clusterUsageTags.clusterWorkers', '2')\n('spark.databricks.clusterUsageTags.clusterPythonVersion', '3')\n('spark.databricks.clusterUsageTags.enableDfAcls', 'false')\n('spark.databricks.cloudfetch.requestDownloadUrlsWithHeaders', 'true')\n('spark.databricks.clusterUsageTags.workerEnvironmentId', 'workerenv-6905323869956636')\n('spark.hadoop.databricks.loki.fileSystemCache.enabled', 'true')\n('spark.shuffle.service.enabled', 'true')\n('spark.hadoop.fs.file.impl', 'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem')\n('spark.plugins', 'org.apache.spark.sql.connect.SparkConnectPlugin')\n('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true')\n('spark.hadoop.fs.cpfs-s3.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem')\n('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0')\n('spark.databricks.clusterUsageTags.clusterId', '0126-064856-zlv8jbbc')\n('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', '')\n('spark.hadoop.fs.s3a.multipart.threshold', '104857600')\n('spark.rpc.message.maxSize', '256')\n('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND_AZURE')\n('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', '')\n('spark.hadoop.fs.fcfs-wasbs.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.databricks.driverNfs.enabled', 'true')\n('spark.databricks.clusterUsageTags.clusterMetastoreAccessType', 'RDS_DIRECT')\n('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false')\n('spark.hadoop.parquet.page.metadata.validation.enabled', 'true')\n('spark.databricks.acl.enabled', 'false')\n('spark.databricks.clusterUsageTags.driverNodeType', 'Standard_D4ads_v5')\n('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName', 'com.databricks.unity.TokenServiceApiTokenProvider')\n('spark.databricks.passthrough.glue.executorServiceFactoryClassName', 'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory')\n('spark.databricks.clusterUsageTags.enableElasticDisk', 'true')\n('spark.databricks.acl.scim.client', 'com.databricks.spark.sql.acl.client.DriverToWebappScimClient')\n('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false')\n('spark.databricks.clusterUsageTags.isSingleUserCluster', 'true')\n('spark.hadoop.fs.adl.impl.disable.cache', 'true')\n('spark.hadoop.parquet.block.size.row.check.max', '10')\n('spark.hadoop.fs.s3a.connection.maximum', '200')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0')\n('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32')\n('spark.shuffle.reduceLocality.enabled', 'false')\n('spark.hadoop.spark.sql.sources.outputCommitterClass', 'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter')\n('spark.hadoop.fs.fcfs-abfs.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.hadoop.databricks.loki.fileStatusCache.enabled', 'true')\n('spark.databricks.clusterUsageTags.clusterGeneration', '3')\n('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true')\n('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false')\n('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000')\n('spark.sql.parquet.compression.codec', 'snappy')\n('spark.hadoop.fs.stage.impl', 'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem')\n('spark.databricks.cloudProvider', 'Azure')\n('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName', 'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider')\n('spark.databricks.cloudfetch.hasRegionSupport', 'true')\n('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories', 'false')\n('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000')\n('spark.databricks.unityCatalog.enabled', 'false')\n('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName', 'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory')\n('spark.sparklyr-backend.threads', '1')\n('spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice', '-1.0')\n('spark.hadoop.fs.fcfs-wasb.impl', 'com.databricks.sql.acl.fs.FixedCredentialsFileSystem')\n('spark.databricks.passthrough.s3a.tokenProviderClassName', 'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider')\n('spark.databricks.session.share', 'false')\n('spark.databricks.clusterUsageTags.clusterResourceClass', 'default')\n('spark.databricks.isShieldWorkspace', 'false')\n('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem')\n('spark.hadoop.fs.dbfs.impl', 'com.databricks.backend.daemon.data.client.DbfsHadoop3')\n('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU')\n('spark.hadoop.fs.gs.impl.disable.cache', 'true')\n('spark.databricks.privateLinkEnabled', 'false')\n('spark.delta.sharing.profile.provider.class', 'io.delta.sharing.DeltaSharingCredentialsProvider')\n('spark.databricks.workspaceUrl', 'adb-6905323869956636.16.azuredatabricks.net')\n('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass', 'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient')\n('spark.hadoop.fs.wasbs.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.databricks.clusterUsageTags.clusterScalingType', 'autoscaling')\n('spark.databricks.automl.serviceEnabled', 'true')\n('spark.hadoop.parquet.page.size.check.estimate', 'false')\n('spark.databricks.clusterUsageTags.attribute_tag_service', '')\n('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class', 'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory')\n('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n('spark.databricks.clusterUsageTags.clusterName', 'eo-general-purpose')\n('spark.databricks.metrics.filesystem_io_metrics', 'true')\n('spark.hadoop.spark.driverproxy.customHeadersToProperties', 'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims')\n('spark.databricks.unityCatalog.credentialScope.enabled', 'true')\n('spark.databricks.cloudfetch.requesterClassName', 'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester')\n('spark.driver.host', '10.2.128.10')\n('spark.databricks.delta.logStore.crossCloud.fatal', 'true')\n('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true')\n('spark.files.fetchFailure.unRegisterOutputOnHost', 'true')\n('spark.databricks.clusterUsageTags.clusterOwnerUserId', '8796140328588637')\n('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false')\n('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0')\n('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER')\n('spark.hadoop.databricks.fs.perfMetrics.enable', 'true')\n('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216')\n('spark.speculation.quantile', '0.9')\n('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false')\n('spark.shuffle.manager', 'SORT')\n('spark.files.overwrite', 'true')\n('spark.databricks.credential.aws.secretKey.redactor', 'com.databricks.spark.util.AWSSecretKeyRedactorProxy')\n('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0')\n('spark.connect.extensions.command.classes', 'io.delta.connect.DeltaCommandPlugin')\n('spark.hadoop.fs.s3.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.repl.class.uri', 'spark://10.2.128.10:45973/classes')\n('spark.master', 'spark://10.2.128.10:7077')\n('spark.hadoop.fs.s3a.impl.disable.cache', 'true')\n('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes', 'false')\n('spark.r.numRBackendThreads', '1')\n('spark.hadoop.fs.wasbs.impl.disable.cache', 'true')\n('spark.hadoop.fs.abfss.impl.disable.cache', 'true')\n('spark.hadoop.fs.azure.cache.invalidator.type', 'com.databricks.encryption.utils.CacheInvalidatorImpl')\n('spark.sql.hive.metastore.version', '0.13.0')\n('spark.shuffle.service.port', '4048')\n('spark.databricks.acl.client', 'com.databricks.spark.sql.acl.client.SparkSqlAclClient')\n('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true')\n('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false')\n('spark.databricks.clusterUsageTags.runtimeEngine', 'STANDARD')\n('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false')\n('spark.databricks.credential.scope.fs.impl', 'com.databricks.sql.acl.fs.CredentialScopeFileSystem')\n('spark.databricks.clusterUsageTags.instanceWorkerEnvId', 'workerenv-6905323869956636')\n('spark.databricks.enablePublicDbfsFuse', 'false')\n('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true')\n('spark.app.id', 'app-20240127175407-0000')\n('spark.databricks.passthrough.adls.tokenProviderClassName', 'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider')\n('spark.app.name', 'Databricks Shell')\n('spark.driver.allowMultipleContexts', 'false')\n('spark.hadoop.fs.AbstractFileSystem.gs.impl', 'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS')\n('spark.databricks.secret.sparkConf.keys.toRedact', '')\n('spark.rdd.compress', 'true')\n('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException', 'false')\n('spark.hadoop.fs.s3a.retry.limit', '6')\n('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', '')\n('spark.databricks.eventLog.dir', 'eventlogs')\n('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName', 'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider')\n('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false')\n('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs')\n('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp')\n('spark.speculation', 'false')\n('spark.hadoop.hive.server2.session.check.interval', '60000')\n('spark.sql.hive.convertCTAS', 'true')\n('spark.connect.extensions.relation.classes', 'io.delta.connect.DeltaRelationPlugin')\n('spark.hadoop.spark.sql.parquet.output.committer.class', 'org.apache.spark.sql.parquet.DirectParquetOutputCommitter')\n('spark.hadoop.fs.s3a.max.total.tasks', '1000')\n('spark.databricks.tahoe.logStore.aws.class', 'com.databricks.tahoe.store.MultiClusterLogStore')\n('spark.hadoop.fs.s3a.fast.upload.default', 'true')\n('spark.databricks.clusterUsageTags.managedResourceGroup', 'databricks-rg-bing-db-with-ssh-rbsv4dg4elgh6')\n('spark.databricks.clusterUsageTags.driverInstanceId', '0ac498cef56d4350843e0918377ce783')\n('spark.hadoop.fs.mlflowdbfs.impl', 'com.databricks.mlflowdbfs.MlflowdbfsFileSystem')\n('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'LEGACY_SINGLE_USER_STANDARD')\n('spark.databricks.eventLog.listenerClassName', 'com.databricks.backend.daemon.driver.DBCEventLoggingListener')\n('spark.hadoop.fs.abfs.impl.disable.cache', 'true')\n('spark.speculation.multiplier', '3')\n('spark.storage.blockManagerTimeoutIntervalMs', '300000')\n('spark.databricks.clusterUsageTags.sparkVersion', '13.3.x-scala2.12')\n('spark.sparkr.use.daemon', 'false')\n('spark.scheduler.listenerbus.eventqueue.capacity', '20000')\n('spark.hadoop.fs.s3a.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark')\n('spark.hadoop.parquet.page.write-checksum.enabled', 'true')\n('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false')\n('spark.hadoop.fs.s3a.threads.max', '136')\n('spark.r.backendConnectionTimeout', '604800')\n('spark.ui.prometheus.enabled', 'true')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs', '0')\n('spark.hadoop.fs.s3n.impl', 'com.databricks.common.filesystem.LokiFileSystem')\n('spark.hadoop.hive.server2.idle.session.timeout', '900000')\n('spark.databricks.redactor', 'com.databricks.spark.util.DatabricksSparkLogRedactorProxy')\n('spark.executor.extraClassPath', '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*')\n('spark.databricks.autotune.maintenance.client.classname', 'com.databricks.maintenanceautocompute.MACClientImpl')\n('spark.app.startTime', '1706378042495')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes', '0')\n('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace', '0')\n('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true')\n('spark.hadoop.parquet.page.verify-checksum.enabled', 'true')\n('spark.logConf', 'true')\n('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true')\n('spark.databricks.clusterUsageTags.driverPublicDns', '20.236.56.179')\n('spark.hadoop.hive.server2.enable.doAs', 'false')\n('spark.hadoop.parquet.filter.columnindex.enabled', 'false')\n('spark.shuffle.memoryFraction', '0.2')\n('spark.databricks.unityCatalog.volumes.fuse.server.enabled', 'true')\n('spark.hadoop.fs.dbfsartifacts.impl', 'com.databricks.backend.daemon.data.client.DBFSV1')\n('spark.hadoop.fs.cpfs-s3a.impl', 'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem')\n('spark.hadoop.fs.s3a.connection.timeout', '50000')\n('spark.databricks.secret.envVar.keys.toRedact', '')\n('spark.databricks.clusterUsageTags.cloudProvider', 'Azure')\n('spark.files.useFetchCache', 'false')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.legacy.charVarcharAsString\", \"true\")\n",
    "transcode(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf83c6e-40f3-4eca-8025-de3a496e6698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_store_sales = spark.read.parquet('wasbs://nds@mahrens.blob.core.windows.net/parquet_sf1000/store_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032275a5-6ef6-4cb2-adaa-dfeeb56b5259",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2879987999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_store_sales.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "nds_gen_to_azure_blob_storage",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}